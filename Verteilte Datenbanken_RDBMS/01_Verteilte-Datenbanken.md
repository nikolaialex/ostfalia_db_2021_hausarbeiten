<a name="verteilte Datenbanken"></a>
# Verteilte Datenbanken

Gem√§√ü [1] wird man haupts√§chlich mit zwei Fragen konfrontiert, wenn Daten auf mehrere Rechner verteilt werden oder in die Cloud gelegt werden sollen.

1.	 Worin liegt der Vorteil der getrennten Datenhaltung gegen√ºber der zentralen Speicherung?
2.	 Wie funktioniert die Verwaltung verteilter Datenbanken bzw. inwieweit muss der Datenbankbenutzer ein Umstieg auf verteilte Systeme umdenken? [1]

<a name="zw√∂lf Regeln"></a>
## Zw√∂lf Regeln f√ºr verteilte Datenbank Systeme (DBS)

Die zw√∂lf ‚ÄûRegeln‚Äú f√ºr Verteilte DBS nach CJ Date [2] stellen die Anforderungen an verteilte Datenbanksysteme dar. Diese Regeln sind in Tabelle x zusammengefasst:

Nummer | Regel
------ | ------
1	| Lokale Autonomie
2	|Keine Abh√§ngigkeiten von zentralen Systemfunktionen
3	|Hohe Verf√ºgbarkeit
4	|Ortstransparenz
5	|Fragmentierungstransparenz
6	|Replikationstransparenz
7	|Verteilte Anfrageverarbeitung
8	|Verteilte Transaktionsverwaltung
9	|Hardware-Unabh√§ngigkeit
10	|Betriebssystemunabh√§ngigkeit
11	|Netzwerkunabh√§ngigkeit
12	|Datenbanksystemunabh√§ngigkeit

Nachfolgend sind diese Regeln genauer beschrieben.
1.	Lokale Autonomie
Jeder Rechner sollte ein Maximum an Kontrolle √ºber die bei ihm gespeicherten Daten haben. Insbesondere sollte der Zugriff auf diese Daten nicht von anderen Rechnern abh√§ngen.
2.	Keine Abh√§ngigkeit von zentralen Systemfunktionen
Zur Unterst√ºtzung einer hohen Knotenautonomie und Verf√ºgbarkeit sollte die Datenbankverarbeitung nicht von zentralen Systemfunktionen abh√§ngen. Solche Komponenten stellen zudem einen potenziellen Leistungsengpass dar.
3.	Hohe Verf√ºgbarkeit
Die Datenbankverarbeitung sollte trotz Fehlern im System (z.B. Rechnerausfall) oder Konfigurations√§nderungen (Installation neuer Software oder Hardware) idealerweise nicht unterbrochen werden.
4.	Ortstransparenz
Die physische Lokation von Datenbankobjekten sollte f√ºr den Benutzer verborgen bleiben. Der Datenzugriff sollte wie auf lokale Objekte m√∂glich sein.
5.	Fragmentierungstransparenz
Eine Relation der Datenbank sollte verteilt an mehreren Knoten gespeichert werden k√∂nnen. Die dabei zugrundeliegende Zerlegung (Fragmentierung) der Relation sollte f√ºr den Datenbankbenutzer transparent bleiben.
6.	Replikationstransparenz
Die replizierte Speicherung von Teilen der Datenbank sollte f√ºr den Benutzer unsichtbar bleiben; die Wartung der Redundanz obliegt ausschlie√ülich der DB-Software.
7.	Verteilte Anfrageverarbeitung
Innerhalb einer DB-Operation sollte auf Daten mehrerer Rechner zugegriffen werden k√∂nnen. Zur effizienten Bearbeitung sind durch das Verteilte DBS geeignete Techniken bereitzustellen (Query-Optimierung).
8.	Verteilte Transaktionsverwaltung
Die Transaktionseigenschaften sind auch bei verteilter Bearbeitung einzuhalten, wozu entsprechende Recovery- und Synchronisationstechniken bereitzustellen sind (verteiltes Commit-Protokoll, Behandlung globaler Deadlocks, u.a.).
9.	Hardware-Unabh√§ngigkeit
Die DB-Verarbeitung sollte auf verschiedenen Hardware-Plattformen m√∂glich sein. S√§mtliche Hardware-Eigenschaften sind f√ºr den DB-Benutzer zu verbergen.
10.	Betriebssystemunabh√§ngigkeit
Die DB-Benutzung sollte unabh√§ngig von den eingesetzten Betriebssystemen sein.
11.	Netzwerkunabh√§ngigkeit
Die verwendeten Kommunikationsprotokolle und -netzwerke sollten ohne Einfluss auf die DB-Verarbeitung bleiben.
12.	Datenbanksystemunabh√§ngigkeit
Es sollte m√∂glich sein, unterschiedliche (heterogene) Datenbanksysteme an den verschiedenen Rechnern einzusetzen, solange sie eine einheitliche Benutzerschnittstelle (z.B. eine gemeinsame SQL-Version) anbieten.
Zus√§tzlich wird auch Leistungstransparenz gefordert. Die Kommunikationsverz√∂gerungen sollen m√∂glichst keine merkliche Verschlechterung der Bearbeitung verursachen. Eine Verbesserung durch die Parallelit√§t der verteilten Bearbeitung wird erwartet. Wie [3] beschreibt, ist es nicht m√∂glich alle zw√∂lf Regeln gleichzeitig zu erf√ºllen. Manche Regeln widersprechen sich gegenseitig. Obwohl die ‚ÄûRegeln‚Äú bereits 30 Jahre alt sind, werden Betriebssystemunabh√§ngigkeit und Hardware-Unabh√§ngigkeit nur teilweise umgesetzt. So wird Microsoft SQL-Server erst seit Ende 2017 auf Linux Betriebssystemen unterst√ºtzt. Offensichtlich entsprechen diese Regeln nicht den W√ºnschen der Hersteller. Andere Regeln (z.B. Nummer 9 und 11) werden inzwischen als selbstverst√§ndlich angesehen.
Schlussendlich m√ºssen Kompromisse geschlossen werden, um das bestm√∂gliche aus den Regeln zu verwenden. In dem nachfolgenden Abschnitt wird auf Verf√ºgbarkeit, sicheren Transaktionsbetrieb und die Verteilung der Daten genauer eingegangen.

## CAP Theorem

Im Jahr 2000 stellte E. Brewer das CAP Theorem auf. Die Begriffe Consistency, Availability, Tolerance of Network Partitions bilden die Abk√ºrzung CAP. Dieses Theorem besagt, dass Konsistenz (Consistency), Verf√ºgbarkeit (Availability) und Ausfalltoleranz (Tolerance of Network Partitions) in verteilten Datenbanken nicht gleichzeitig erf√ºllt werden k√∂nnen. Wie bereits bei den zw√∂lf Regeln nach CJ Date deutlich wurde, k√∂nnen nicht alle Regeln erf√ºllt werden. Das CAP Theorem sagt sogar aus, dass nur zwei von den drei genannten Eigenschaften (Consistency, Availablity, Tolerance of Network Partitions) vollst√§ndig realisiert werden k√∂nnen.

_Consistency_: Zur gleichen Zeit m√ºssen alle mehrfach gehaltenen g√ºltigen Daten √ºbereinstimmen. Damit die Daten gleichzeitig aktualisiert werden k√∂nnen, wird Atomarit√§t aus dem ACID-Modell ben√∂tigt. Die Konsistenz ist in einem Transaktionsbetrieb eine zwingende Vorrausetzung. In der achten Regel von CJ Date (siehe Kapitel Zw√∂lf Regeln von Date) festgelegt. [1]

_Availability:_ Es k√∂nnen hier die Regeln von Date Nummer 3 und Nummer 7 verwendet werden. Ein Ausfall einzelner Rechner darf nicht zum Ausfall des gesamten Systems f√ºhren. Eine Anfrage an einen Rechnerknoten muss immer zu einem Ergebnis f√ºhren. Neben der Erreichbarkeit ist auch eine kurze Antwortzeit von Bedeutung. Verf√ºgbarkeit ist im Transaktionsbetrieb besonders wichtig. [1]

_Tolerance of Network Partitions:_ Es k√∂nnen Daten in Netzen verloren gehen oder versp√§tet zugestellt werden. Laut [1] soll das Gesamtsystem tolerant mit diesen Problemen umgehen. Ein System sollte bei Konsistenzverletzungen nicht ausfallen oder komplett zur√ºckgesetzt werden m√ºssen.

Da nur zwei von drei genannten Eigenschaften vollst√§ndig realisiert werden k√∂nnen, sind in Abbildung die Schnittmengen der drei Eigenschaften dargestellt.

<p align="center">
<img src="images/CAP-Schicker.png" width=250>
</p>

### CA-Systeme
Bei relationalen Datenbanksystemen findet h√§ufig das CA-System Anwendung. Bei dem CA-System werden nur Konsistenz und Verf√ºgbarkeit verwendet. Schon seit mehr als 20 Jahren wird zum Beispiel das Zwei-Phasen-Commit-Protokoll in verteilten Datenbanken mit gro√üem Erfolg eingesetzt.[1] Auf PA und CP-Systeme wird in dieser Auswertung nicht weiter eingegangen, da sich ausschlie√ülich mit Relationalen Datenbanken befasst wird. 

## Zwei-Phasen-Commit-Protokoll
Laut [1] ist es in hochverfuÃàgbaren verteilten Systemen schwer die zeitnahe Konsistenz zu garantieren. Seit vielen Jahren wird in relationalen Datenbanken das zwei-Phasen-Commit-Protokoll verwendet. Mit Hilfe von diesem Protokoll kann die Konsistenz eines Systems auch √ºber mehrere Rechner hinweg sichergestellt werden. In einer relationalen Datenbank entspricht das Zwei-Phasen-Commit Protokoll einem CA-System (siehe Abschnitt CA-System). Es kann somit hohe Verf√ºgbarkeit und zeitnahe Konsistenz garantiert werden. H√§ufig findet dieses Protokoll in betriebswirtschaftlichen Anwendungen sein Einsatzgebiet (Ein- und Verkauf, Produktion oder in Buchungs- und Abrechnungssystemen). XOpen/DTD ist als Standard f√ºr Zwei Phasen-Commit-Protokoll verwendet. Beim Zwei-Phasen-Commit Protokoll werden zwei Phasen ausgef√ºhrt. Die Wahlphase und die Entscheidungsphase. In der folgenden Abbildung ist das Commit Protokoll laut [4] grafisch dargestellt.

<p align="center">
<img src="images/2Phasen.png" width=250>
</p>

Wie in der Abbildung zu sehen, fragt der Koordinator in der Wahlphase, die Teilnehmer, ob diese ein Commit durchf√ºhren k√∂nnen. Als n√§chsten Schritt teilen die Teilnehmer dem Koordinator ihre Entscheidung mit. In der zweiten Phase, der Entscheidungsphase, entscheidet der Koordinator, indem er die erhaltenen Ergebnisse der Teilnehmer auswertet. Die Knoten (Teilnehmer), die mit ‚ÄûJa‚Äú geantwortet haben, warten auf eine Entscheidung. Wie [4] beschreibt, ben√∂tigt das Basisprotokoll pro weiteren Teilnehmer vier Nachrichten, die vom Koordinator verschickt werden, sodass eine √ºber n Knoten verteilte globale Transaktion insgesamt im Erfolgsfall 4(ùëõ‚àí1) Nachrichten ben√∂tigt.

## Teilung von Datenmengen
Die ersten Ans√§tze zur Teilung der Datenmengen und Aufw√§nde bringen horizontale (zeilenweise) und vertikale (spaltenweise) Verteilung. W√§hrend in der horizontalen Verteilung die Gesamtmenge √ºber mehrere identisch strukturierte Tabellen nach gewissen Merkmalen (z.B. nach Anfangsbuchstaben der Nachnamen, Geburtsmonat, Geburtstag) verteilt wird, werden in vertikaler Verteilung die Daten segmentiert. So k√∂nnen Teile der zusammenh√§ngenden Information ausgelagert werden. Seit Beginn der 1980er Jahre war es Thema mehrerer wissenschaftlichen Arbeiten √ºber die verschiedenen Modelle zur Festlegung der Selektionspr√§dikate [3].

Beide Verteilungen weisen auch negative Eigenschaften auf. Die vertikale Verteilung erfordert weitere Aufteilung der Relationen, welche mit zus√§tzlichem Aufwand verbunden ist. Bei der horizontalen Verteilung werden die Ressourcen nicht gleichm√§√üig belegt. Damit ist ein einigerma√üen gleichm√§√üiges Loadbalancing nur schwer m√∂glich. Die Operationen auf die Gesamtmenge der Relation muss deswegen sorgsamer √ºberdacht werden. Auch Kombination der Fragmentierungen, beide Verteilungen, ist m√∂glich. Durch die integrierte Logik des verteilten DBMS kann die Aufteilung automatisiert werden und f√ºr die Abfragen vollst√§ndig transparent gehalten werden.

Die Datenbanksystemunabh√§ngigkeit ist nur teilweise gegeben, da die Hersteller f√ºr die Abfragen √∂fter SQL-Dialekte verwenden.
Nach wie vor ist die Hauptursache der meisten Ausf√§lle im Datenbankumfeld die Hardware [5]. In der horizontalen Verteilung  w√§re der Hardware-Ausfall noch m√∂glicherweise nicht vollst√§ndig gesch√§ftssch√§digend, da nur eine Teilmenge betroffen w√§re. In der vertikalen Verteilung w√§re der Ausfall deutlich gravierender.







