{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wordcount_mit_Spark_RDD",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM6arb37/kSRPSEdaIO5rV9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristianKitte/SparkProjekt/blob/main/notebook/Wordcount_mit_Spark_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJtej7pTMlmr"
      },
      "source": [
        "# Vorbereitung des Notebooks\n",
        "Vor der eigentlichen Umsetzung der Aufgabe muss das Notebook vorbereitet werden. ***Python*** und die ***wichtigsten Bibliotheken sind bereits Bestandteil*** der\n",
        "von Colab bereit gestellten Installation. \n",
        "\n",
        "Da Spark mit Java programmiert wurde, wird als erstes ***Java installiert***. In dieser Anwendung wird das Open JDK in der Version 8 verwendet. Die Installation\n",
        "erfolgt direkt auf der Linux Ebene.\n",
        "\n",
        "Nachdem Java installiert ist, kann Spark selbst ***heruntergeladen sowie alle Systemvariablen gesetzt werden***. Auch hier wird mit Linuxbefehlen operiert. In dieser Anwendung wird die aktuell neueste Version 3.2 verwendet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREA6gqNAMs8",
        "outputId": "38dbfad9-ee35-4e71-8912-9825c01d8a81"
      },
      "source": [
        "# Installation  von Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "print(\"Java ist installiert...\")\n",
        "\n",
        "# Download und Entpacken von Spark (Versionsnummer anpassen!)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "print(\"Spark ist verfügbar...\")\n",
        "\n",
        "# Setzen der Systemvariablen für Java und Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "print(\"Umgebungsvariablen sind gesetzt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java ist installiert...\n",
            "Spark ist verfügbar...\n",
            "Umgebungsvariablen sind gesetzt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWPQDeb8bl9M",
        "outputId": "32a34b56-1132-4292-ac64-3cdbcc026e0f"
      },
      "source": [
        "# Installation von findspark und pyspark\n",
        "\n",
        "!pip install findspark\n",
        "print(\"FindSpark wurde installiert...\")\n",
        "\n",
        "!pip install pyspark\n",
        "print(\"PySpark wurde installiert...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "FindSpark wurde installiert...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "PySpark wurde installiert...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9u5_oWDb7sw",
        "outputId": "790735a1-e42c-4b21-dbcb-4715d0708074"
      },
      "source": [
        "# Initialisieren von findspark\n",
        "\n",
        "try: \n",
        "  import findspark\n",
        "  from pyspark import SparkContext, SparkConf\n",
        "  \n",
        "  findspark.init()\n",
        "  \n",
        "  print(\"FindSpark und PySpark wurden initialisiert\")\n",
        "except ImportError: \n",
        "  raise ImportError(\"Fehler bei der Initialiserung von FindSpark und PySpark\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FindSpark und PySpark wurden initialisiert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjc0YuSDjZvn"
      },
      "source": [
        "# Einlesen und Vorbereiten der Textdatei\n",
        "\n",
        "Im ersten Abschnitt werden zunächst zwei Methoden definiert.\n",
        "\n",
        "* Der ersten Methode ***get_file_from_url*** werden als Parameter eine ***URL*** sowie ein ***Speicherort*** angegeben. Bei Ihrem Aufruf lädt die Methode eine Datei von der angegebenen URL herunter und speichert sie in Google Drive ab.\n",
        "\n",
        "* Die zweite Methode ***cut_file*** nimmt als Parameter einen ***numerischen Start- und Endwert*** sowie die Angabe einer ***Quell- und Zieldatei*** entgegen. Bei Ihrem Aufruf entfernt die Methode alle Zeilen vor bzw. nach den durch Start- und Endwert definierten Zeilenbereich aus der Quelldatei und speichert das Ergebnis in die Zieldatei.\n",
        "\n",
        "In dem folgenden Block wird dann im Anschluss die Datei mit den gesammelten Werken von Shakespeare von der Seite des MIT ***heruntergeladen*** sowie ***von nicht benötigten Zeilen bereinigt*** und in einer ***neuen Datei*** gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaQNN9jkIie",
        "outputId": "b13caeb7-b629-4ce7-aac5-36440f608a5e"
      },
      "source": [
        "# Erstellen einer Methode, um Dateien aus dem Internet zu laden und zu speichern\n",
        "\n",
        "import requests \n",
        "\n",
        "def get_file_from_url(file_url, place_to_save):\n",
        "  try:\n",
        "    req = requests.get(file_url, stream = True) \n",
        "\n",
        "    with open(place_to_save, \"wb\") as file: \n",
        "\t    for block in req.iter_content(chunk_size = 1024): \n",
        "\t\t    if block: \n",
        "\t\t\t    file.write(block) \n",
        "     \n",
        "    print(\"Die Datei wurde herunter geladen und angelegt: {}\".format(file_url))\n",
        "  \n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))   \n",
        "\n",
        "print(\"Die Funktion get_file_from_url wurde angelegt...\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion get_file_from_url wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf1_kePZkL4_",
        "outputId": "f3b3f019-b71f-4a83-aed9-a8c2c0c2ff61"
      },
      "source": [
        "# Erstellen einer Methode, um eine Textdatei am Anfang und am Ende um die jeweils\n",
        "# angegebene Zahl an Reihen zu beschneiden.\n",
        "\n",
        "def cut_file(anfang, ende, quelldatei, zieldatei):\n",
        "  try:\n",
        "    with open(quelldatei, \"r\") as source:\n",
        "      lines = source.readlines()\n",
        "    \n",
        "    source.close()\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Start: {}\".format(anfang))\n",
        "    print(\"Ende: {}\".format(ende))\n",
        "    print(\"\")\n",
        "\n",
        "    current_count = 0\n",
        "  \n",
        "    with open(zieldatei, \"w\") as target:\n",
        "      for line in lines:\n",
        "        if current_count >= anfang and current_count <= ende:\n",
        "          target.write(line)\n",
        "\n",
        "        current_count = current_count + 1   \n",
        "    \n",
        "    target.close()\n",
        "\n",
        "    print(\"Datei wurde beschnitten...\")\n",
        "\n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))\n",
        "\n",
        "print(\"Die Funktion cut_file wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion cut_file wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9qv_PqBlOVt",
        "outputId": "99a88bf1-849e-4144-b1e3-6df27c0bfacd"
      },
      "source": [
        "# Datei von der Quelle nach Colab laden\n",
        "\n",
        "file_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "place_to_save = \"/content/shakespeare.txt\"\n",
        "\n",
        "get_file_from_url(file_url, place_to_save)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Datei wurde vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Datei wurde herunter geladen und angelegt: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
            "\n",
            "Datei wurde vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HObiLObj8Yi",
        "outputId": "b3b19e3c-935c-4c3e-f29a-1cc152af0bf3"
      },
      "source": [
        "# Unnötige Zeilen am Ende und am Start entfernen\n",
        "\n",
        "file_source = \"/content/shakespeare.txt\"\n",
        "file_target = \"/content/shakespeare_neu.txt\"\n",
        "\n",
        "cut_file(244,124438,file_source, file_target)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Die Arbeitsdatei ist vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start: 244\n",
            "Ende: 124438\n",
            "\n",
            "Datei wurde beschnitten...\n",
            "\n",
            "Die Arbeitsdatei ist vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6HwX_ITMzWh"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "Um mit Spark arbeiten zu können, muss als erstes eine Verbindung zu Spark in Form eines ***SparkContext*** aufgebaut werden. In dem hier verwendeten Code wird ein SparkContext erzeugt, welcher die Bezeichnung ***WordCounter*** erhält. Er soll lokal laufen und hierbei parallel ***alle verfügbaren Kerne*** verwenden. Dieser Block kann in einer Anwendung nur einmal ausgeführt werden.\n",
        "\n",
        "Anschließend wird die Textdatei eingelesen und gibt ein ***RDD*** in Form einer Liste von String zurück. In diesen Fall entsprechen die Strings den ***Zeilen der Textdatei***. Die Methode ***map*** führt auf jedes Element des zugrunde liegenden RDD - also den Zeilen der Textdatei - die angegebene Funktion aus.\n",
        "\n",
        "In dem hier vorliegenden Fall findet zunächst eine Reihe von Ersetzungen (***replace***), dann eine Konvertierung in Kleinbuchstaben (***lower***) und am Schluss eine Filterung (***filter***) auf leere Zeilen statt. Als Ergebnis wird ein neues RDD vom Typ String zurückgegeben. Das ursprüngliche RDD wird ***nicht verändert***. Es ist ***immutable***. Die Verwendung einer FluentApi bewirkt eine übersichtliche Strukturierung des Codes.\n",
        "\n",
        "In der folgenden Codesequenze wird jedes Listenelement des RDD durch ***flatMap*** in seine einzelnen Wörter aufgeteilt. Für jedes Wort wird ein Tupel erzeugt und zurückgegeben. Da es sich um eine flatMap handelt, verfügt das zurück gegebene RDD nur noch über eine sehr lange ***Liste von Tupel***. Die Funktion ***reduceByKey*** merged im Anschluss die einzelnen Tupel. Als Ergebnis erhält man eine ***Liste von Tupel*** mit ***eindeutigen Wörtern und deren Vorkommen***.\n",
        "\n",
        "Mit der Methode ***sortBy*** wird auf die ***Anzahl der Wortvorkommen sortiert***. Das zurück gegebene RDD ***sorted_counts*** kann im Anschluss ausgegeben werden, nachdem mit ***collect*** alle Werte eingesammelt wurden."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGSUceqCASSH",
        "outputId": "dc83da25-a7e1-4731-de1a-659e574b033f"
      },
      "source": [
        "# Erzeugen eines Spark Kontext \n",
        "\n",
        "sc = SparkContext(\"local[*]\",\"WordCounter\")\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "print(\"Der Spark Kontext wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Der Spark Kontext wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ikM4w-OlR7n",
        "outputId": "d2e8630c-998b-424a-a951-7d2e2d788bbd"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "from pyspark import SparkContext \n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import SQLContext \n",
        "from pyspark.sql import DataFrameReader\n",
        "\n",
        "lines=sc.textFile(file_target) \\\n",
        "  .map( lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower()) \\\n",
        "  .filter(lambda linex: linex.strip() != \"\") \n",
        "\n",
        "top_out = 30\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der ersten {} Zeilen des Textes\".format(top_out))\n",
        "print(\"\")\n",
        "\n",
        "for line in lines.collect()[0:top_out]:\n",
        "  print(line) \n",
        "\n",
        "words = lines.flatMap(lambda line: line.split(\" \")) \\\n",
        "  .map(lambda word: (word, 1)) \\\n",
        "  .reduceByKey(lambda a,b:a+b)\n",
        "\n",
        "sorted_counts = words.sortBy(lambda wordCounts: wordCounts[1], ascending=False)\n",
        "\n",
        "top_length = 30\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der {} größten Vorkommen\".format(top_length))\n",
        "print(\"\")\n",
        "\n",
        "i = 0\n",
        "for word, count in sorted_counts.collect()[0:top_length]:\n",
        "    print(\"{} : {} : {} \".format(i, word, count))\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Ausgabe der ersten 30 Zeilen des Textes\n",
            "\n",
            "1609\n",
            "the sonnets\n",
            "by william shakespeare\n",
            "                     1\n",
            "  from fairest creatures we desire increase \n",
            "  that thereby beauty's rose might never die \n",
            "  but as the riper should by time decease \n",
            "  his tender heir might bear his memory:\n",
            "  but thou contracted to thine own bright eyes \n",
            "  feed'st thy light's flame with self substantial fuel \n",
            "  making a famine where abundance lies \n",
            "  thy self thy foe  to thy sweet self too cruel:\n",
            "  thou that art now the world's fresh ornament \n",
            "  and only herald to the gaudy spring \n",
            "  within thine own bud buriest thy content \n",
            "  and tender churl mak'st waste in niggarding:\n",
            "    pity the world  or else this glutton be \n",
            "    to eat the world's due  by the grave and thee \n",
            "                     2\n",
            "  when forty winters shall besiege thy brow \n",
            "  and dig deep trenches in thy beauty's field \n",
            "  thy youth's proud livery so gazed on now \n",
            "  will be a tattered weed of small worth held:  \n",
            "  then being asked  where all thy beauty lies \n",
            "  where all the treasure of thy lusty days;\n",
            "  to say within thine own deep sunken eyes \n",
            "  were an all eating shame  and thriftless praise \n",
            "  how much more praise deserved thy beauty's use \n",
            "  if thou couldst answer 'this fair child of mine\n",
            "  shall sum my count  and make my old excuse'\n",
            "\n",
            "Ausgabe der 30 größten Vorkommen\n",
            "\n",
            "0 :  : 669971 \n",
            "1 : the : 27507 \n",
            "2 : and : 26705 \n",
            "3 : i : 20191 \n",
            "4 : to : 19294 \n",
            "5 : of : 18076 \n",
            "6 : a : 14502 \n",
            "7 : you : 12957 \n",
            "8 : my : 12468 \n",
            "9 : that : 10950 \n",
            "10 : in : 10903 \n",
            "11 : is : 9473 \n",
            "12 : not : 8443 \n",
            "13 : for : 8181 \n",
            "14 : with : 7965 \n",
            "15 : it : 7212 \n",
            "16 : be : 6963 \n",
            "17 : me : 6962 \n",
            "18 : your : 6865 \n",
            "19 : his : 6825 \n",
            "20 : this : 6276 \n",
            "21 : but : 6267 \n",
            "22 : he : 6102 \n",
            "23 : as : 5927 \n",
            "24 : have : 5838 \n",
            "25 : thou : 5378 \n",
            "26 : so : 4948 \n",
            "27 : will : 4858 \n",
            "28 : him : 4625 \n",
            "29 : by : 4386 \n"
          ]
        }
      ]
    }
  ]
}