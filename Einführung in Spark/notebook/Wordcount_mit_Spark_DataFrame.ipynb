{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wordcount_mit_Spark_DataFrame",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMpHOBjc++cWR8/D6DGD1EN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChristianKitte/SparkProjekt/blob/main/notebook/Wordcount_mit_Spark_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJtej7pTMlmr"
      },
      "source": [
        "# Vorbereitung des Notebooks\n",
        "Vor der eigentlichen Umsetzung der Aufgabe muss das Notebook vorbereitet werden. ***Python*** und die ***wichtigsten Bibliotheken sind bereits Bestandteil*** der\n",
        "von Colab bereit gestellten Installation. \n",
        "\n",
        "Da Spark mit Java programmiert wurde, wird als erstes ***Java installiert***. In dieser Anwendung wird das Open JDK in der Version 8 verwendet. Die Installation\n",
        "erfolgt direkt auf der Linux Ebene.\n",
        "\n",
        "Nachdem Java installiert ist, kann Spark selbst ***heruntergeladen sowie alle Systemvariablen gesetzt werden***. Auch hier wird mit Linuxbefehlen operiert. In dieser Anwendung wird die aktuell neueste Version 3.2 verwendet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VREA6gqNAMs8",
        "outputId": "dd67ee95-77a1-4a83-cc35-7b4c40c0cdc3"
      },
      "source": [
        "# Installation  von Java\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "print(\"Java ist installiert...\")\n",
        "\n",
        "# Download und Entpacken von Spark (Versionsnummer anpassen!)\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.0-bin-hadoop3.2.tgz\n",
        "\n",
        "print(\"Spark ist verfügbar...\")\n",
        "\n",
        "# Setzen der Systemvariablen für Java und Spark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.0-bin-hadoop3.2\"\n",
        "\n",
        "print(\"Umgebungsvariablen sind gesetzt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Java ist installiert...\n",
            "Spark ist verfügbar...\n",
            "Umgebungsvariablen sind gesetzt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWPQDeb8bl9M",
        "outputId": "6d51c60b-7512-4856-cc4b-b12b503cd613"
      },
      "source": [
        "# Installation von findspark und pyspark\n",
        "\n",
        "!pip install findspark\n",
        "print(\"FindSpark wurde installiert...\")\n",
        "\n",
        "!pip install pyspark\n",
        "print(\"PySpark wurde installiert...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.7/dist-packages (1.4.2)\n",
            "FindSpark wurde installiert...\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "PySpark wurde installiert...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9u5_oWDb7sw",
        "outputId": "d2463898-5890-4c91-e9d2-6b42e09a0d72"
      },
      "source": [
        "# Initialisieren von findspark\n",
        "\n",
        "try: \n",
        "  import findspark\n",
        "    \n",
        "  findspark.init()\n",
        "  \n",
        "  print(\"FindSpark wurde initialisiert\")\n",
        "except ImportError: \n",
        "  raise ImportError(\"Fehler bei der Initialiserung von FindSpark\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FindSpark wurde initialisiert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjc0YuSDjZvn"
      },
      "source": [
        "# Einlesen und Vorbereiten der Textdatei\n",
        "\n",
        "Im ersten Abschnitt werden zunächst zwei Methoden definiert.\n",
        "\n",
        "* Der ersten Methode ***get_file_from_url*** werden als Parameter eine ***URL*** sowie ein ***Speicherort*** angegeben. Bei Ihrem Aufruf lädt die Methode eine Datei von der angegebenen URL herunter und speichert sie in Google Drive ab.\n",
        "\n",
        "* Die zweite Methode ***cut_file*** nimmt als Parameter einen ***numerischen Start- und Endwert*** sowie die Angabe einer ***Quell- und Zieldatei*** entgegen. Bei Ihrem Aufruf entfernt die Methode alle Zeilen vor bzw. nach den durch Start- und Endwert definierten Zeilenbereich aus der Quelldatei und speichert das Ergebnis in die Zieldatei.\n",
        "\n",
        "In dem folgenden Block wird dann im Anschluss die Datei mit den gesammelten Werken von Shakespeare von der Seite des MIT ***heruntergeladen*** sowie ***von nicht benötigten Zeilen bereinigt*** und in einer ***neuen Datei*** gespeichert."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNaQNN9jkIie",
        "outputId": "3a20949d-5ae9-43fb-c29e-a3b42bedf4d9"
      },
      "source": [
        "# Erstellen einer Methode, um Dateien aus dem Internet zu laden und zu speichern\n",
        "\n",
        "import requests \n",
        "\n",
        "def get_file_from_url(file_url, place_to_save):\n",
        "  try:\n",
        "    req = requests.get(file_url, stream = True) \n",
        "\n",
        "    with open(place_to_save, \"wb\") as file: \n",
        "\t    for block in req.iter_content(chunk_size = 1024): \n",
        "\t\t    if block: \n",
        "\t\t\t    file.write(block) \n",
        "     \n",
        "    print(\"Die Datei wurde herunter geladen und angelegt: {}\".format(file_url))\n",
        "  \n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))   \n",
        "\n",
        "print(\"Die Funktion get_file_from_url wurde angelegt...\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion get_file_from_url wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf1_kePZkL4_",
        "outputId": "ac55f268-8c4a-498f-d00f-62f432335906"
      },
      "source": [
        "# Erstellen einer Methode, um eine Textdatei am Anfang und am Ende um die jeweils\n",
        "# angegebene Zahl an Reihen zu beschneiden.\n",
        "\n",
        "def cut_file(anfang, ende, quelldatei, zieldatei):\n",
        "  try:\n",
        "    with open(quelldatei, \"r\") as source:\n",
        "      lines = source.readlines()\n",
        "    \n",
        "    source.close()\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Start: {}\".format(anfang))\n",
        "    print(\"Ende: {}\".format(ende))\n",
        "    print(\"\")\n",
        "\n",
        "    current_count = 0\n",
        "  \n",
        "    with open(zieldatei, \"w\") as target:\n",
        "      for line in lines:\n",
        "        if current_count >= anfang and current_count <= ende:\n",
        "          target.write(line)\n",
        "\n",
        "        current_count = current_count + 1   \n",
        "    \n",
        "    target.close()\n",
        "\n",
        "    print(\"Datei wurde beschnitten...\")\n",
        "\n",
        "  except ValueError:\n",
        "    print(\"Fehler {}\".format(ValueError))\n",
        "\n",
        "print(\"Die Funktion cut_file wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Funktion cut_file wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9qv_PqBlOVt",
        "outputId": "cc2a1e07-7523-415e-ebe4-070acab6abd9"
      },
      "source": [
        "# Datei von der Quelle nach Colab laden\n",
        "\n",
        "file_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "place_to_save = \"/content/shakespeare.txt\"\n",
        "\n",
        "get_file_from_url(file_url, place_to_save)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Datei wurde vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Datei wurde herunter geladen und angelegt: https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
            "\n",
            "Datei wurde vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HObiLObj8Yi",
        "outputId": "5a33060c-0902-4ce0-ad19-58f7854bce8a"
      },
      "source": [
        "# Unnötige Zeilen am Ende und am Start entfernen\n",
        "\n",
        "file_source = \"/content/shakespeare.txt\"\n",
        "file_target = \"/content/shakespeare_neu.txt\"\n",
        "\n",
        "cut_file(244,124438,file_source, file_target)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Die Arbeitsdatei ist vorbereitet...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start: 244\n",
            "Ende: 124438\n",
            "\n",
            "Datei wurde beschnitten...\n",
            "\n",
            "Die Arbeitsdatei ist vorbereitet...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6HwX_ITMzWh"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "Anschließend wird die Textdatei eingelesen. Die Funktion ***Session.read.text*** liest eine Textdatei ein und gibt\n",
        "ein ***direkt nutzbares typisiertes Spark DataFrame zurück***. Über ein Reflexion-Prozess kann das ***Schema der enthaltenen Daten*** - in diesem Fall ein nullable String – ***erkannt und ausgegeben*** werden. \n",
        "\n",
        "Im Weiteren finden zunächst eine Reihe von Ersetzungen (***replace***), dann eine Konvertierung in Kleinbuchstaben (***lower***) \n",
        "und am Schluss eine Filterung (***filter***) auf leere Zeilen statt. Hierbei wird jedes Mal ein ***neues DataFrame*** \n",
        "zurückgegeben. Da Spark DataFrames auf RDDs basieren, sind auch sie ***immutable***. Die Methode ***withColumn*** bewirkt, dass \n",
        "die ***übergebene Funktion*** ähnlich der Map Funktion ***auf alle Datensätze angewendet wird***.\n",
        "\n",
        "Der letzte Schritt erinnert stark an ein SQL Konstrukt. Zunächst wird jede Zeile durch ihre ***Leerzeichen gesplittet***. \n",
        "Die Funktion ***explode*** sorgt dafür, dass das so entstandene ***Array mit n Spalten*** als ein ***Array mit einer Spalte (value2) \n",
        "und n Reihen*** zurückgegeben wird.\n",
        "\n",
        "Die Funktion ***groupBy*** gruppiert die in ***value2*** enthaltenen Werte (***Wörter***) und ***Count*** aggregiert die Anzahl der einzelnen \n",
        "Vorkommen. Abgeschlossen wir die Anweisung mit einem ***sort*** und der Ausgabe der sortierten Liste. Die Nutzung der \n",
        "Fluent API macht den Code hierbei gut lesbar. Ebenso fällt die Ähnlichkeit zu Panda DataFrames auf."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGSUceqCASSH",
        "outputId": "929e9514-8ff0-4ad9-9755-77435667fe32"
      },
      "source": [
        "# Erzeugen einer Spark Session\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "session = SparkSession.builder.appName(\"Wordcount\").getOrCreate()\n",
        "\n",
        "print(\"Die Spark Session wurde angelegt...\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Spark Session wurde angelegt...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ikM4w-OlR7n",
        "outputId": "5f8ed51f-e60a-4394-cd06-c3f6209ed9f9"
      },
      "source": [
        "# Auszählen der Wörter\n",
        "\n",
        "import pyspark.sql.functions as func\n",
        "\n",
        "df = session.read.text(file_target)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Schema der eingelesenen Datei:\")\n",
        "df.printSchema()\n",
        "print(\"\")\n",
        "\n",
        "top_out = 30\n",
        "top_length = 30\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der ersten {} Zeilen des Textes\".format(top_out))\n",
        "print(\"\")\n",
        "\n",
        "df.show(n=top_out,truncate=False)\n",
        "\n",
        "df=df.withColumn('value', func.translate('value', ',', ' '))\n",
        "df=df.withColumn('value', func.translate('value', '.', ' '))\n",
        "df=df.withColumn('value', func.translate('value', '-', ' '))\n",
        "df=df.withColumn('value', func.lower('value'))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Ausgabe der {} größten Vorkommen\".format(top_length))\n",
        "print(\"\")\n",
        "\n",
        "df=df.withColumn('value2',func.explode(func.split(func.col('value'), ' ')))\\\n",
        "  .groupBy('value2')\\\n",
        "  .count()\\\n",
        "  .sort('count', ascending=False)\\\n",
        "  .show(n=top_length,truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Schema der eingelesenen Datei:\n",
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n",
            "\n",
            "\n",
            "Ausgabe der ersten 30 Zeilen des Textes\n",
            "\n",
            "+-------------------------------------------------------+\n",
            "|value                                                  |\n",
            "+-------------------------------------------------------+\n",
            "|1609                                                   |\n",
            "|                                                       |\n",
            "|THE SONNETS                                            |\n",
            "|                                                       |\n",
            "|by William Shakespeare                                 |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                     1                                 |\n",
            "|  From fairest creatures we desire increase,           |\n",
            "|  That thereby beauty's rose might never die,          |\n",
            "|  But as the riper should by time decease,             |\n",
            "|  His tender heir might bear his memory:               |\n",
            "|  But thou contracted to thine own bright eyes,        |\n",
            "|  Feed'st thy light's flame with self-substantial fuel,|\n",
            "|  Making a famine where abundance lies,                |\n",
            "|  Thy self thy foe, to thy sweet self too cruel:       |\n",
            "|  Thou that art now the world's fresh ornament,        |\n",
            "|  And only herald to the gaudy spring,                 |\n",
            "|  Within thine own bud buriest thy content,            |\n",
            "|  And tender churl mak'st waste in niggarding:         |\n",
            "|    Pity the world, or else this glutton be,           |\n",
            "|    To eat the world's due, by the grave and thee.     |\n",
            "|                                                       |\n",
            "|                                                       |\n",
            "|                     2                                 |\n",
            "|  When forty winters shall besiege thy brow,           |\n",
            "|  And dig deep trenches in thy beauty's field,         |\n",
            "|  Thy youth's proud livery so gazed on now,            |\n",
            "|  Will be a tattered weed of small worth held:         |\n",
            "+-------------------------------------------------------+\n",
            "only showing top 30 rows\n",
            "\n",
            "\n",
            "Ausgabe der 30 größten Vorkommen\n",
            "\n",
            "+------+------+\n",
            "|value2|count |\n",
            "+------+------+\n",
            "|      |679850|\n",
            "|the   |27507 |\n",
            "|and   |26705 |\n",
            "|i     |20191 |\n",
            "|to    |19294 |\n",
            "|of    |18076 |\n",
            "|a     |14502 |\n",
            "|you   |12957 |\n",
            "|my    |12468 |\n",
            "|that  |10950 |\n",
            "|in    |10903 |\n",
            "|is    |9473  |\n",
            "|not   |8443  |\n",
            "|for   |8181  |\n",
            "|with  |7965  |\n",
            "|it    |7212  |\n",
            "|be    |6963  |\n",
            "|me    |6962  |\n",
            "|your  |6865  |\n",
            "|his   |6825  |\n",
            "|this  |6276  |\n",
            "|but   |6267  |\n",
            "|he    |6102  |\n",
            "|as    |5927  |\n",
            "|have  |5838  |\n",
            "|thou  |5378  |\n",
            "|so    |4948  |\n",
            "|will  |4858  |\n",
            "|him   |4625  |\n",
            "|by    |4386  |\n",
            "+------+------+\n",
            "only showing top 30 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovHhBQquBVwD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}